#!/bin/bash

set -e  # Exit on any error

# Store project root directory
PROJECT_ROOT="$(pwd)"

# Work directory is the same as project root (no temp directory)
TEST_WORKDIR="$PROJECT_ROOT"

# Prepend bin directory to PATH so easy-db-lab uses this worktree's version
export PATH="$PROJECT_ROOT/bin:$PATH"

# Use sandbox-admin AWS profile for S3 operations
export AWS_PROFILE=sandbox-admin

# Track current step for resume functionality
CURRENT_STEP=0
CURRENT_STEP_NAME=""
STEP_LOG=$(mktemp /tmp/e2e-step-XXXXXX)

# Parse command line arguments (must be before cleanup trap setup)
WAIT_BEFORE_TEARDOWN=false
BUILD_IMAGE=false
ENABLE_SPARK=false
ENABLE_CASSANDRA=false
ENABLE_CLICKHOUSE=false
ENABLE_OPENSEARCH=false
USE_EBS=false
EXIT_OK=false
LIST_STEPS=false
BREAKPOINTS=""
while [[ $# -gt 0 ]]; do
  case $1 in
    --list-steps|-l)
      LIST_STEPS=true
      shift
      ;;
    --break)
      BREAKPOINTS="$2"
      shift 2
      ;;
    --wait)
      WAIT_BEFORE_TEARDOWN=true
      shift
      ;;
    --build)
      BUILD_IMAGE=true
      shift
      ;;
    --spark)
      ENABLE_SPARK=true
      shift
      ;;
    --cassandra)
      ENABLE_CASSANDRA=true
      shift
      ;;
    --clickhouse)
      ENABLE_CLICKHOUSE=true
      shift
      ;;
    --opensearch)
      ENABLE_OPENSEARCH=true
      shift
      ;;
    --all)
      ENABLE_SPARK=true
      ENABLE_CASSANDRA=true
      ENABLE_CLICKHOUSE=true
      ENABLE_OPENSEARCH=true
      shift
      ;;
    --ebs)
      USE_EBS=true
      shift
      ;;
    *)
      echo "Unknown option: $1"
      echo "Usage: $0 [--list-steps|-l] [--break <steps>] [--wait] [--build] [--spark] [--cassandra] [--clickhouse] [--opensearch] [--all] [--ebs]"
      exit 1
      ;;
  esac
done

# ============================================================================
# Test Step Functions
# ============================================================================

step_build_project() {
    echo "=== Building project ==="
    if [ "$ENABLE_SPARK" = true ]; then
        # Pre-build cassandra-analytics with JDK 11 for bulk-writer
        echo "=== Pre-building Cassandra Analytics (JDK 11) ==="
        "$PROJECT_ROOT/bin/build-cassandra-analytics"
        (cd "$PROJECT_ROOT" && ./gradlew clean shadowJar installDist :spark-connector-test1:jar :bulk-writer:jar)
    else
        (cd "$PROJECT_ROOT" && ./gradlew clean shadowJar installDist)
    fi
}

step_check_version() {
    echo "=== Checking version ==="
    easy-db-lab version
}

step_build_image() {
    if [ "$BUILD_IMAGE" = true ]; then
        echo "=== Building packer images ==="
        easy-db-lab build-image
    else
        echo "=== Skipping packer image build (use --build to enable) ==="
    fi
}

step_set_policies() {
    echo "=== Creating IAM managed policies ==="
    "$PROJECT_ROOT/bin/set-policies" --group-name EasyDBLabUsers --profile sandbox-admin
}

step_init_cluster() {
    echo "=== Initializing cluster ==="
    local spark_opts=""
    if [ "$ENABLE_SPARK" = true ]; then
        spark_opts="--spark.enable --spark.master.instance.type m5.xlarge --spark.worker.instance.type m5.xlarge --spark.worker.instance.count 2"
        echo "=== Spark provisioning enabled ==="
    fi
    local ebs_opts=""
    if [ "$USE_EBS" = true ]; then
        ebs_opts="--ebs.type gp3 --ebs.size 256"
        echo "=== EBS volumes enabled (gp3, 256GB) ==="
    fi
    local created_timestamp=$(date +%s)
    easy-db-lab init -c 3 -i c5.2xlarge test --clean --up -s 1 --tag "created=$created_timestamp" --cidr 10.14.0.0/20 $spark_opts $ebs_opts
}

step_setup_kubectl() {
    echo "=== Setting up kubectl access ==="
    source env.sh
}

step_wait_k3s_ready() {
    echo "=== Checking K3s cluster connectivity ==="
    if kubectl get nodes &>/dev/null; then
        echo "kubectl connectivity established"
        return 0
    fi
    echo "ERROR: kubectl failed to connect to K3s cluster"
    return 1
}

step_verify_k3s() {
    echo "=== Verifying K3s cluster nodes ==="
    kubectl get nodes

    echo "=== Verifying K3s system pods ==="
    kubectl get pods -A

    echo "=== Waiting for all nodes to be Ready ==="
    kubectl wait --for=condition=Ready nodes --all --timeout=120s
}

step_verify_vpc_tags() {
    echo "=== Verifying VPC tags ==="

    # Extract VPC ID from state.json
    local vpc_id=$(jq -r '.vpcId' state.json)
    if [ -z "$vpc_id" ] || [ "$vpc_id" = "null" ]; then
        echo "ERROR: VPC ID not found in state.json"
        return 1
    fi

    echo "VPC ID: $vpc_id"

    # Get VPC tags using AWS CLI
    local tags=$(aws ec2 describe-vpcs --vpc-ids "$vpc_id" --query 'Vpcs[0].Tags' --output json)
    echo "VPC Tags: $tags"

    # Verify easy_cass_lab system tag is present
    local easy_cass_lab_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "easy_cass_lab") | .Value')
    if [ "$easy_cass_lab_tag" = "1" ]; then
        echo "  easy_cass_lab tag: OK (value: $easy_cass_lab_tag)"
    else
        echo "ERROR: easy_cass_lab tag not found or incorrect"
        return 1
    fi

    # Verify ClusterId system tag is present
    local cluster_id_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "ClusterId") | .Value')
    if [ -n "$cluster_id_tag" ] && [ "$cluster_id_tag" != "null" ]; then
        echo "  ClusterId tag: OK (value: $cluster_id_tag)"
    else
        echo "ERROR: ClusterId tag not found"
        return 1
    fi

    # Verify user-supplied 'created' tag is present (from init --tag "created=$timestamp")
    local created_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "created") | .Value')
    if [ -n "$created_tag" ] && [ "$created_tag" != "null" ]; then
        echo "  created tag: OK (value: $created_tag)"
    else
        echo "ERROR: User-supplied 'created' tag not found on VPC"
        echo "This tag was specified via 'init --tag created=...' and should be propagated to the VPC"
        return 1
    fi

    echo "=== VPC tag verification passed ==="
}

step_list_hosts() {
    echo "=== Listing cluster hosts ==="
    easy-db-lab hosts

    echo "=== Status Without Cassandra Version ==="
    easy-db-lab status

    echo "=== Listing available Cassandra versions ==="
    easy-db-lab cassandra list
}

step_test_mcp_server() {
    echo "=== Testing MCP server /status endpoint ==="

    # Remove stale .mcp.json if present
    rm -f .mcp.json

    # Kill any stale server on this port
    local server_port=9852
    local stale_pid=$(lsof -ti :$server_port 2>/dev/null || true)
    if [ -n "$stale_pid" ]; then
        echo "Killing stale process on port $server_port (PID: $stale_pid)"
        kill "$stale_pid" 2>/dev/null || true
        sleep 2
    fi

    # Start the MCP server in the background on a fixed port
    easy-db-lab server --port $server_port &
    local server_pid=$!
    echo "$server_pid" > .mcp-server.pid
    local server_url="http://127.0.0.1:${server_port}"

    # Wait for the server to accept connections
    local timeout=120
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
        if curl -s -o /dev/null "${server_url}/status" 2>/dev/null; then
            echo "MCP server ready at: $server_url"
            break
        fi
        sleep 2
        elapsed=$((elapsed + 2))
    done

    if [ $elapsed -ge $timeout ]; then
        echo "ERROR: MCP server did not become ready within ${timeout}s"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Call /status endpoint
    echo "=== Calling /status endpoint ==="
    local http_code
    local response
    response=$(curl -s -w "\n%{http_code}" "${server_url}/status")
    http_code=$(echo "$response" | tail -1)
    local body
    body=$(echo "$response" | sed '$d')

    echo "HTTP status code: $http_code"
    echo "Response body (first 20 lines):"
    echo "$body" | head -20

    # Verify HTTP 200
    if [ "$http_code" != "200" ]; then
        echo "ERROR: Expected HTTP 200, got $http_code"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify response contains cluster data
    if echo "$body" | jq -e '.cluster' > /dev/null 2>&1; then
        echo "Response contains cluster data: OK"
    else
        echo "ERROR: Response does not contain expected cluster data"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify response contains nodes data
    if echo "$body" | jq -e '.nodes' > /dev/null 2>&1; then
        echo "Response contains nodes data: OK"
    else
        echo "ERROR: Response does not contain expected nodes data"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify S3 section contains fullpath field and save for later steps
    if echo "$body" | jq -e '.s3.fullpath' > /dev/null 2>&1; then
        local fullpath=$(echo "$body" | jq -r '.s3.fullpath')
        echo "$fullpath" > .s3-fullpath
        echo "Response contains s3.fullpath: OK ($fullpath)"
    else
        echo "ERROR: Response does not contain expected s3.fullpath field"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Clean up
    echo "=== Stopping MCP server ==="
    kill -9 "$server_pid" 2>/dev/null || true
    rm -f .mcp.json .mcp-server.pid

    echo "=== MCP server /status test passed ==="
}

step_verify_s3_backup() {
    echo "=== Verifying S3 config backup ==="

    local s3_fullpath=$(cat .s3-fullpath)
    local s3_bucket=$(jq -r '.s3Bucket' state.json)

    echo "S3 Fullpath: $s3_fullpath"

    # Verify key backup files exist
    local missing_files=()

    echo "=== Checking kubeconfig backup ==="
    if aws s3 ls "s3://${s3_fullpath}/config/kubeconfig" > /dev/null 2>&1; then
        echo "  config/kubeconfig: OK"
    else
        echo "  config/kubeconfig: MISSING"
        missing_files+=("config/kubeconfig")
    fi

    echo "=== Checking config backups ==="
    for config_file in "config/cassandra_versions.yaml" "config/environment.sh" "config/setup_instance.sh"; do
        if aws s3 ls "s3://${s3_fullpath}/${config_file}" > /dev/null 2>&1; then
            echo "  ${config_file}: OK"
        else
            echo "  ${config_file}: MISSING"
            missing_files+=("${config_file}")
        fi
    done

    echo "=== Listing all S3 backup contents ==="
    aws s3 ls "s3://${s3_bucket}/" --recursive | head -30

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing backup files in s3://${s3_fullpath}/:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== S3 backup verification passed ==="
}

step_setup_cassandra() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra setup (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Setting Cassandra version to 5.0 ==="
    easy-db-lab cassandra use 5.0

    echo "=== Updating configuration ==="
    easy-db-lab cassandra update-config
}

step_verify_cassandra_backup() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra backup verification (use --cassandra to enable) ==="
        return 0
    fi

    echo "=== Verifying Cassandra config backup to S3 ==="

    local s3_fullpath=$(cat .s3-fullpath)

    local missing_files=()

    echo "=== Checking cassandra.patch.yaml backup ==="
    if aws s3 ls "s3://${s3_fullpath}/config/cassandra.patch.yaml" > /dev/null 2>&1; then
        echo "  config/cassandra.patch.yaml: OK"
    else
        echo "  config/cassandra.patch.yaml: MISSING"
        missing_files+=("config/cassandra.patch.yaml")
    fi

    echo "=== Checking cassandra config directory backup ==="
    local cassandra_config_count=$(aws s3 ls "s3://${s3_fullpath}/config/cassandra-config/" --recursive 2>/dev/null | wc -l)
    if [ "$cassandra_config_count" -gt 0 ]; then
        echo "  config/cassandra-config: OK ($cassandra_config_count files)"
    else
        echo "  config/cassandra-config: MISSING"
        missing_files+=("config/cassandra-config/")
    fi

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing Cassandra backup files in s3://${s3_fullpath}/:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== Cassandra backup verification passed ==="
}

step_verify_restore() {
    echo "=== Verifying restore from VPC ID ==="

    # Extract VPC ID from state.json
    local vpc_id=$(jq -r '.vpcId' state.json)
    if [ -z "$vpc_id" ] || [ "$vpc_id" = "null" ]; then
        echo "ERROR: VPC ID not found in state.json"
        return 1
    fi

    echo "VPC ID: $vpc_id"

    # Create temporary restore directory
    local restore_dir=$(mktemp -d)
    echo "=== Using temporary restore directory: $restore_dir ==="

    # Change into restore directory and run status with EASY_DB_LAB_RESTORE_VPC env var
    echo "=== Running status with EASY_DB_LAB_RESTORE_VPC to trigger restore ==="
    (
        cd "$restore_dir"
        EASY_DB_LAB_RESTORE_VPC="$vpc_id" easy-db-lab status
    )

    # Verify restored files
    local missing_files=()

    echo "=== Verifying restored files ==="

    # Check state.json was created
    if [ -f "$restore_dir/state.json" ]; then
        echo "  state.json: OK"
    else
        echo "  state.json: MISSING"
        missing_files+=("state.json")
    fi

    # Check kubeconfig was restored
    if [ -f "$restore_dir/kubeconfig" ]; then
        echo "  kubeconfig: OK"
    else
        echo "  kubeconfig: MISSING"
        missing_files+=("kubeconfig")
    fi

    # Check environment.sh was restored
    if [ -f "$restore_dir/environment.sh" ]; then
        echo "  environment.sh: OK"
    else
        echo "  environment.sh: MISSING"
        missing_files+=("environment.sh")
    fi

    # Check setup_instance.sh was restored
    if [ -f "$restore_dir/setup_instance.sh" ]; then
        echo "  setup_instance.sh: OK"
    else
        echo "  setup_instance.sh: MISSING"
        missing_files+=("setup_instance.sh")
    fi

    # Check cassandra_versions.yaml was restored
    if [ -f "$restore_dir/cassandra_versions.yaml" ]; then
        echo "  cassandra_versions.yaml: OK"
    else
        echo "  cassandra_versions.yaml: MISSING"
        missing_files+=("cassandra_versions.yaml")
    fi

    # Verify Cassandra-specific files if Cassandra was enabled
    if [ "$ENABLE_CASSANDRA" = true ]; then
        if [ -f "$restore_dir/cassandra.patch.yaml" ]; then
            echo "  cassandra.patch.yaml: OK"
        else
            echo "  cassandra.patch.yaml: MISSING"
            missing_files+=("cassandra.patch.yaml")
        fi

        local cassandra_config_count=$(find "$restore_dir/cassandra" -type f 2>/dev/null | wc -l)
        if [ "$cassandra_config_count" -gt 0 ]; then
            echo "  cassandra/: OK ($cassandra_config_count files)"
        else
            echo "  cassandra/: MISSING or empty"
            missing_files+=("cassandra/")
        fi
    fi

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing restored files in $restore_dir:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== Restore verification passed ==="
}

step_cassandra_start_stop() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra start/stop cycle (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Starting Cassandra ==="
    easy-db-lab cassandra start

    echo "=== Status With Cassandra Running ==="
    easy-db-lab status

    echo "=== Stopping Cassandra ==="
    easy-db-lab cassandra stop

    echo "=== Waiting for Cassandra to stop ==="
    sleep 10

    echo "=== Starting Cassandra again ==="
    easy-db-lab cassandra start

    echo "=== Restarting Cassandra ==="
    easy-db-lab cassandra restart
}

step_test_ssh_nodetool() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping SSH nodetool test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Testing SSH access and nodetool via env.sh aliases ==="
    ssh db0 nodetool status
}

step_check_sidecar() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Sidecar check (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Check Sidecar ==="
    ssh db0 "curl -s http://$(easy-db-lab ip db0 --private):9043/api/v1/cassandra/schema" | jq 'keys'
}

step_test_exec() {
    echo "=== Testing exec command (sequential) ==="
    easy-db-lab exec -t cassandra hostname

    echo "=== Testing exec command (parallel) ==="
    easy-db-lab exec -t cassandra -p uptime

    echo "=== Testing exec command (with host filter) ==="
    easy-db-lab exec -t cassandra --hosts db0,db1 date
}

step_stress_test() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping stress test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Running stress test ==="
    ssh app0 "bash -l -c 'cassandra-easy-stress run KeyValue -d 10s'"
}

step_stress_k8s() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping stress K8s test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Starting stress job on K8s ==="
    easy-db-lab cassandra stress run --name e2e-test -- KeyValue -d 30s

    echo "=== Checking stress job status ==="
    easy-db-lab cassandra stress status

    echo "=== Waiting for stress job to complete ==="
    local timeout=120
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
        local status=$(kubectl get jobs -l app.kubernetes.io/name=cassandra-easy-stress -o jsonpath='{.items[0].status.succeeded}' 2>/dev/null || echo "")
        if [ "$status" = "1" ]; then
            echo "Stress job completed successfully"
            break
        fi
        echo "Waiting for stress job to complete... ($elapsed/$timeout seconds)"
        sleep 10
        elapsed=$((elapsed + 10))
    done

    echo "=== Viewing stress job logs ==="
    # Get the job name that we just created (most recent)
    local job_name=$(kubectl get jobs -l app.kubernetes.io/name=cassandra-easy-stress --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1].metadata.name}')
    if [ -n "$job_name" ]; then
        easy-db-lab cassandra stress logs "$job_name" --tail 50
    fi

    echo "=== Stopping stress job ==="
    easy-db-lab cassandra stress stop --all

    echo "=== Verifying stress jobs are deleted ==="
    easy-db-lab cassandra stress status
}

step_spark_submit() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping Spark job submission (use --spark to enable) ==="
        return 0
    fi
    echo "=== Submitting Spark job to EMR ==="
    easy-db-lab spark submit \
      --jar "$PROJECT_ROOT/spark-connector-test1/build/libs/spark-connector-test1-12.jar" \
      --main-class com.rustyrazorblade.easydblab.spark.KeyValuePrefixCount \
      --args "$(easy-db-lab ip db0 --private)" \
      --wait
}

step_spark_status() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping Spark status check (use --spark to enable) ==="
        return 0
    fi
    echo "=== Listing Spark jobs ==="
    easy-db-lab spark jobs

    echo "=== Checking Spark job status (automatically downloads logs) ==="
    easy-db-lab spark status

    echo "=== Downloading all EMR logs (standalone command) ==="
    easy-db-lab spark logs
}

step_bulk_writer_direct() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping bulk writer test (use --spark to enable) ==="
        return 0
    fi
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping bulk writer test (use --cassandra to enable for table creation) ==="
        return 0
    fi

    echo "=== Creating bulk writer test table ==="
    easy-db-lab cassandra cql "CREATE KEYSPACE IF NOT EXISTS bulk_test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"
    easy-db-lab cassandra cql "CREATE TABLE IF NOT EXISTS bulk_test.data (id bigint PRIMARY KEY, course blob, marks bigint)"

    echo "=== Submitting bulk writer (direct mode) to EMR ==="
    local sidecar_hosts=$(easy-db-lab hosts --type cassandra --private | tr '\n' ',' | sed 's/,$//')

    easy-db-lab spark submit \
      --jar "$PROJECT_ROOT/bulk-writer/build/libs/bulk-writer.jar" \
      --main-class com.rustyrazorblade.easydblab.spark.DirectBulkWriter \
      --args "$sidecar_hosts" bulk_test data datacenter1 10000 4 \
      --wait

    echo "=== Verifying bulk write results ==="
    easy-db-lab cassandra cql "SELECT COUNT(*) FROM bulk_test.data"
}

step_clickhouse_start() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse deployment (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Initializing ClickHouse configuration ==="
    easy-db-lab clickhouse init

    echo "=== Deploying ClickHouse cluster ==="
    easy-db-lab clickhouse start

    echo "=== Checking ClickHouse cluster status ==="
    easy-db-lab clickhouse status

    echo "=== Waiting for ClickHouse pods to be ready ==="
    kubectl wait --for=condition=Ready pods --all -n clickhouse --timeout=300s

    echo "=== Verifying ClickHouse pods ==="
    kubectl get pods -n clickhouse
}

step_clickhouse_test() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse test (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Testing ClickHouse connectivity ==="

    clickhouse-query "SELECT version()"

    clickhouse-query <<'EOF'
CREATE OR REPLACE TABLE test (
    id UInt64,
    updated_at DateTime DEFAULT now(),
    updated_at_date Date DEFAULT toDate(updated_at)
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/test', '{replica}')
ORDER BY id
SETTINGS storage_policy = 's3_main'
EOF

    clickhouse-query "INSERT INTO test (id) VALUES (1)"
}

step_clickhouse_stop() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse stop (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Stopping ClickHouse cluster ==="
    easy-db-lab clickhouse stop --force

    echo "=== Verifying ClickHouse namespace is deleted ==="
    local timeout=60
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
      if ! kubectl get namespace clickhouse &>/dev/null; then
        echo "ClickHouse namespace successfully deleted"
        return 0
      fi
      echo "Waiting for ClickHouse namespace to be deleted... ($elapsed/$timeout seconds)"
      sleep 5
      elapsed=$((elapsed + 5))
    done
}

step_opensearch_start() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch deployment (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Deploying OpenSearch domain (this takes 10-30 minutes) ==="
    easy-db-lab opensearch start --wait

    echo "=== Checking OpenSearch domain status ==="

    # should be ready
    easy-db-lab opensearch status
}

step_opensearch_test() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch test (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Testing OpenSearch connectivity ==="

    # Get endpoint using --endpoint flag
    local endpoint=$(easy-db-lab opensearch status --endpoint)
    if [ -z "$endpoint" ]; then
        echo "ERROR: Could not get OpenSearch endpoint"
        return 1
    fi

    echo "=== OpenSearch endpoint: $endpoint ==="

    # Test cluster health via control node
    echo "=== Testing cluster health ==="
    ssh control0 "curl -s -k https://${endpoint}/_cluster/health" | jq .

    # Create test index
    echo "=== Creating test index ==="
    ssh control0 "curl -s -k -X PUT https://${endpoint}/test-index"

    # Insert test document
    echo "=== Inserting test document ==="
    ssh control0 "curl -s -k -X POST https://${endpoint}/test-index/_doc/1 -H 'Content-Type: application/json' -d '{\"message\": \"hello from e2e test\"}'"

    # Verify document
    echo "=== Verifying test document ==="
    ssh control0 "curl -s -k https://${endpoint}/test-index/_doc/1" | jq .
}

step_opensearch_stop() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch stop (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Stopping OpenSearch domain ==="
    easy-db-lab opensearch stop --force
}

step_test_observability() {
    echo "=== Testing VictoriaMetrics and VictoriaLogs ==="

    # Wait for VictoriaMetrics to be ready
    echo "=== Waiting for VictoriaMetrics pod to be ready ==="
    kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=victoriametrics --timeout=120s

    # Wait for VictoriaLogs to be ready
    echo "=== Waiting for VictoriaLogs pod to be ready ==="
    kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=victorialogs --timeout=120s

    # Test VictoriaMetrics health endpoint
    echo "=== Testing VictoriaMetrics health ==="
    local vm_health=$(ssh control0 "curl -s http://localhost:8428/health")
    echo "VictoriaMetrics health: $vm_health"

    # Test VictoriaLogs health endpoint
    echo "=== Testing VictoriaLogs health ==="
    local vl_health=$(ssh control0 "curl -s http://localhost:9428/health")
    echo "VictoriaLogs health: $vl_health"

    # Query VictoriaMetrics for metrics (should have some metrics from OTel)
    echo "=== Querying VictoriaMetrics for metrics ==="
    local metrics_count=$(ssh control0 "curl -s 'http://localhost:8428/api/v1/label/__name__/values' | jq '.data | length'")
    echo "Number of metric names in VictoriaMetrics: $metrics_count"
    if [ "$metrics_count" -lt 1 ]; then
        echo "WARNING: No metrics found in VictoriaMetrics yet (may need more time for OTel to send data)"
    fi

    # Query VictoriaLogs to check it's accepting connections
    echo "=== Testing VictoriaLogs query endpoint ==="
    local vl_query=$(ssh control0 "curl -s -o /dev/null -w '%{http_code}' 'http://localhost:9428/select/logsql/query?query=*'")
    echo "VictoriaLogs query endpoint response code: $vl_query"
    if [ "$vl_query" != "200" ]; then
        echo "WARNING: VictoriaLogs query endpoint returned unexpected status: $vl_query"
    fi

    # Test Grafana is up and datasources are configured
    echo "=== Testing Grafana health ==="
    local grafana_health=$(ssh control0 "curl -s http://localhost:3000/api/health | jq -r '.database'")
    echo "Grafana database status: $grafana_health"

    echo "=== Testing Grafana datasources ==="
    local datasources=$(ssh control0 "curl -s http://localhost:3000/api/datasources | jq -r '.[].name'")
    echo "Configured datasources:"
    echo "$datasources"

    # Verify VictoriaMetrics datasource exists
    if echo "$datasources" | grep -q "VictoriaMetrics"; then
        echo "VictoriaMetrics datasource: OK"
    else
        echo "ERROR: VictoriaMetrics datasource not found!"
        return 1
    fi

    # Verify VictoriaLogs datasource exists
    if echo "$datasources" | grep -q "VictoriaLogs"; then
        echo "VictoriaLogs datasource: OK"
    else
        echo "ERROR: VictoriaLogs datasource not found!"
        return 1
    fi

    echo "=== Observability stack test completed ==="
}

step_test_logs_query() {
    echo "=== Testing logs query command ==="

    # Query logs using the CLI - this tests the full pipeline:
    # Vector -> VictoriaLogs -> VictoriaLogsService -> CLI output
    echo "=== Querying all logs (should have systemd/journald logs) ==="
    local log_output=$(easy-db-lab logs query --since 1h --limit 10 2>&1) || true
    echo "$log_output"

    # Check if we got any results (not just error messages)
    if echo "$log_output" | grep -q "Found .* log entries"; then
        echo "=== Logs query returned results: OK ==="
    elif echo "$log_output" | grep -q "No logs found"; then
        echo "WARNING: No logs found yet - Vector may need more time to ingest"
    elif echo "$log_output" | grep -q "Failed to query logs"; then
        echo "ERROR: Logs query failed"
        echo "$log_output"
        return 1
    fi

    # If Spark is enabled, check for EMR logs
    if [ "$ENABLE_SPARK" = true ]; then
        echo "=== Querying EMR logs ==="
        easy-db-lab logs query --source emr --since 1h --limit 5 || true
    fi

    echo "=== Logs query test completed ==="
}

step_test_metrics_backup() {
    echo "=== Testing VictoriaMetrics backup ==="

    # Run metrics backup command
    easy-db-lab metrics backup

    local s3_fullpath=$(cat .s3-fullpath)

    # Verify backup was created in S3
    echo "=== Checking VictoriaMetrics backup in S3 ==="
    local backup_count=$(aws s3 ls "s3://${s3_fullpath}/victoriametrics/" 2>/dev/null | wc -l)
    if [ "$backup_count" -gt 0 ]; then
        echo "  victoriametrics backup: OK ($backup_count entries)"
        aws s3 ls "s3://${s3_fullpath}/victoriametrics/" | head -5
    else
        echo "ERROR: VictoriaMetrics backup not found in S3"
        return 1
    fi

    echo "=== VictoriaMetrics backup test completed ==="
}

step_test_logs_backup() {
    echo "=== Testing VictoriaLogs backup ==="

    # Run logs backup command
    easy-db-lab logs backup

    local s3_fullpath=$(cat .s3-fullpath)

    # Verify backup was created in S3
    echo "=== Checking VictoriaLogs backup in S3 ==="
    local backup_count=$(aws s3 ls "s3://${s3_fullpath}/victorialogs/" 2>/dev/null | wc -l)
    if [ "$backup_count" -gt 0 ]; then
        echo "  victorialogs backup: OK ($backup_count entries)"
        aws s3 ls "s3://${s3_fullpath}/victorialogs/" | head -5
    else
        echo "ERROR: VictoriaLogs backup not found in S3"
        return 1
    fi

    echo "=== VictoriaLogs backup test completed ==="
}

step_teardown() {
    # Kill stale MCP server if still running
    if [ -f .mcp-server.pid ]; then
        local pid=$(cat .mcp-server.pid)
        kill "$pid" 2>/dev/null || true
        rm -f .mcp-server.pid
    fi
    rm -f .s3-fullpath
    rm -f "$STEP_LOG"

    echo "=== Tearing down cluster ==="
    easy-db-lab down --yes

    echo "=== End-to-end test completed successfully ==="
}

# ============================================================================
# Step Registry and Runner
# ============================================================================

# Steps that run from project root (before cd to TEST_WORKDIR)
STEPS_PROJECT_ROOT=(
    "step_build_project:Build project"
    "step_check_version:Check version"
    "step_build_image:Build packer image"
)

# Steps that run from TEST_WORKDIR
STEPS_WORKDIR=(
    "step_set_policies:Set IAM policies"
    "step_init_cluster:Initialize cluster"
    "step_setup_kubectl:Setup kubectl"
    "step_wait_k3s_ready:Wait for K3s"
    "step_verify_k3s:Verify K3s cluster"
    "step_verify_vpc_tags:Verify VPC tags"
    "step_list_hosts:List hosts"
    "step_test_mcp_server:Test MCP server status endpoint"
    "step_verify_s3_backup:Verify S3 backup"
    "step_setup_cassandra:Setup Cassandra"
    "step_verify_cassandra_backup:Verify Cassandra backup"
    "step_verify_restore:Verify restore from VPC"
    "step_cassandra_start_stop:Cassandra start/stop cycle"
    "step_test_ssh_nodetool:Test SSH and nodetool"
    "step_check_sidecar:Check Sidecar"
    "step_test_exec:Test exec command"
    "step_stress_test:Run stress test"
    "step_stress_k8s:Run stress K8s test"
    "step_spark_submit:Submit Spark job"
    "step_spark_status:Check Spark status"
    "step_bulk_writer_direct:Test bulk writer (direct)"
    "step_clickhouse_start:Start ClickHouse"
    "step_clickhouse_test:Test ClickHouse"
    "step_clickhouse_stop:Stop ClickHouse"
    "step_opensearch_start:Start OpenSearch"
    "step_opensearch_test:Test OpenSearch"
    "step_opensearch_stop:Stop OpenSearch"
    "step_test_observability:Test observability stack"
    "step_test_logs_query:Test logs query command"
    "step_test_metrics_backup:Test VictoriaMetrics backup"
    "step_test_logs_backup:Test VictoriaLogs backup"
    "step_teardown:Teardown cluster"
)

# Combined steps for resume
ALL_STEPS=("${STEPS_PROJECT_ROOT[@]}" "${STEPS_WORKDIR[@]}")
PROJECT_ROOT_STEP_COUNT=${#STEPS_PROJECT_ROOT[@]}

list_steps() {
    echo "Available steps:"
    echo ""
    local total_steps=${#ALL_STEPS[@]}
    for ((i=0; i<total_steps; i++)); do
        local step_entry="${ALL_STEPS[$i]}"
        local step_name="${step_entry#*:}"
        printf "  %2d) %s\n" "$((i+1))" "$step_name"
    done
}

is_breakpoint() {
    local step_num=$1
    if [ -z "$BREAKPOINTS" ]; then
        return 1
    fi
    IFS=',' read -ra BP_ARRAY <<< "$BREAKPOINTS"
    for bp in "${BP_ARRAY[@]}"; do
        if [ "$bp" = "$step_num" ]; then
            return 0
        fi
    done
    return 1
}

run_from_step() {
    local start_step=${1:-0}
    local total_steps=${#ALL_STEPS[@]}
    local end_step=${2:-$total_steps}  # Optional end step (exclusive), defaults to all

    for ((i=start_step; i<end_step; i++)); do
        CURRENT_STEP=$i
        local step_entry="${ALL_STEPS[$i]}"
        local step_func="${step_entry%%:*}"
        local step_name="${step_entry#*:}"
        CURRENT_STEP_NAME="$step_name"

        echo ""
        echo "=========================================="
        echo "Step $((i+1))/$total_steps: $step_name"
        echo "=========================================="

        # Check for breakpoint
        if is_breakpoint "$((i+1))"; then
            echo ""
            echo ">>> BREAKPOINT at step $((i+1)): $step_name"
            read -p "Press Enter to continue..."
            echo ""
        fi

        # Determine which directory to run in and execute step
        # Disable errexit temporarily for controlled error handling
        local result
        local setup_kubectl_index=$((PROJECT_ROOT_STEP_COUNT + 2))  # step_setup_kubectl position
        set +e
        if [ $i -lt $PROJECT_ROOT_STEP_COUNT ]; then
            pushd "$PROJECT_ROOT" > /dev/null
            $step_func 2>&1 | tee "$STEP_LOG"
            result=${PIPESTATUS[0]}
            popd > /dev/null
        else
            pushd "$TEST_WORKDIR" > /dev/null
            # Only source env.sh for steps after step_setup_kubectl (index 5)
            # Earlier steps either don't need it or source it themselves
            if [ $i -gt $setup_kubectl_index ] && [ -f env.sh ]; then
                source env.sh
            fi
            $step_func 2>&1 | tee "$STEP_LOG"
            result=${PIPESTATUS[0]}
            popd > /dev/null
        fi
        set -e
        if [ $result -ne 0 ]; then
            echo ""
            echo "=========================================="
            echo "FAILED: Step $((i+1)) - $step_name (exit code: $result)"
            echo "=========================================="
            echo "Last 30 lines of output:"
            echo "------------------------------------------"
            tail -30 "$STEP_LOG"
            echo "------------------------------------------"
            echo "Full log: $STEP_LOG"
            return 1
        fi
    done

    return 0
}

# Export variables for use in subshells (e.g., zsh session)
export PROJECT_ROOT TEST_WORKDIR

# ============================================================================
# Cleanup Function
# ============================================================================

cleanup() {
    local exit_code=$?

    # Success case - exit cleanly
    if [ "$EXIT_OK" = true ]; then
        rm -f "$STEP_LOG"
        echo ""
        echo "=== All tests passed successfully ==="
        exit 0
    fi

    if [ "$WAIT_BEFORE_TEARDOWN" != true ]; then
        while true; do
            echo ""
            echo "=== Test work directory: $TEST_WORKDIR ==="
            if [ -n "$CURRENT_STEP_NAME" ]; then
                echo "Failed at step $((CURRENT_STEP+1)): $CURRENT_STEP_NAME"
            fi
            if [ -f "$STEP_LOG" ]; then
                echo ""
                echo "Last 30 lines of output:"
                echo "------------------------------------------"
                tail -30 "$STEP_LOG"
                echo "------------------------------------------"
                echo "Full log: $STEP_LOG"
            fi
            echo ""
            echo "What would you like to do?"
            echo "  1) Retry from failed step (resume test)"
            echo "  2) Start a shell session in the directory"
            echo "  3) Tear down environment (easy-db-lab down --yes)"
            echo "  4) Exit"
            echo ""
            read -p "Choose [1-4]: " -n 1 -r choice
            echo
            case $choice in
                1)
                    if [ -n "$CURRENT_STEP" ]; then
                        echo "=== Resuming from step $((CURRENT_STEP+1)): $CURRENT_STEP_NAME ==="
                        if run_from_step "$CURRENT_STEP"; then
                            echo "=== Test completed successfully ==="
                            exit_code=0
                            break
                        else
                            echo "=== Step failed again ==="
                        fi
                    else
                        echo "No step to retry."
                    fi
                    ;;
                2)
                    echo "=== Starting shell session in $TEST_WORKDIR ==="
                    echo "Available commands:"
                    echo "  easy-db-lab  - Run easy-db-lab commands"
                    echo "  rebuild      - Rebuild the project (shadowJar + installDist)"
                    echo "  rerun        - Rebuild and resume from failed step"
                    echo "Type 'exit' to return to cleanup menu"
                    echo ""
                    (
                        cd "$TEST_WORKDIR"

                        # Create temp zsh config directory
                        TEMP_ZDOTDIR=$(mktemp -d)

                        # Create custom .zshrc with our functions
                        cat > "$TEMP_ZDOTDIR/.zshrc" << ZSHRC_EOF
# Easy DB Lab shell session

# easy-db-lab is available via PATH (set by parent script)

# Rebuild function - runs gradle in project root
rebuild() {
    echo "=== Rebuilding project ==="
    (cd "\$PROJECT_ROOT" && ./gradlew shadowJar installDist)
    echo "=== Rebuild complete ==="
}

# Rerun function - rebuild and resume from failed step
rerun() {
    echo "=== Rebuilding project ==="
    (cd "\$PROJECT_ROOT" && ./gradlew shadowJar installDist)
    if [ \$? -ne 0 ]; then
        echo "=== Rebuild failed ==="
        return 1
    fi
    echo "=== Rebuild complete ==="
    echo "=== Resuming from step $((CURRENT_STEP+1)): $CURRENT_STEP_NAME ==="
    echo "NOTE: For full resume with proper environment, exit shell and choose option 1"
}

# Source env.sh if it exists in current directory
if [ -f env.sh ]; then
    source env.sh
fi

# Source user's real zshrc for comfort (prompt, etc.) but ignore errors
[ -f ~/.zshrc ] && source ~/.zshrc 2>/dev/null || true
ZSHRC_EOF

                        # Export variables for the new shell
                        export PROJECT_ROOT
                        export ZDOTDIR="$TEMP_ZDOTDIR"

                        # Start zsh with our custom config
                        exec zsh
                    )
                    # Loop back to menu after shell exits
                    ;;
                3)
                    echo "=== Tearing down environment ==="
                    (
                        cd "$TEST_WORKDIR"
                        if [ -f env.sh ]; then
                            source env.sh
                        fi
                        easy-db-lab down --yes
                    )
                    echo "=== Environment torn down ==="
                    # Loop back to menu for directory cleanup
                    ;;
                4)
                    echo "=== Exiting ==="
                    break
                    ;;
                *)
                    echo "Invalid choice. Please enter 1, 2, 3, or 4."
                    ;;
            esac
        done
    fi
    exit $exit_code
}
trap cleanup EXIT

# ============================================================================
# Main Execution
# ============================================================================

# Handle --list-steps flag
if [ "$LIST_STEPS" = true ]; then
    list_steps
    EXIT_OK=true
    exit 0
fi

# Change to work directory
cd "$TEST_WORKDIR"
echo "=== Work directory: $(pwd) ==="

# Handle --wait flag for inspection before teardown
if [ "$WAIT_BEFORE_TEARDOWN" = true ]; then
    # Run all steps except teardown (last step)
    TOTAL_STEPS=${#ALL_STEPS[@]}
    LAST_STEP=$((TOTAL_STEPS - 1))  # Teardown is the last step
    run_from_step 0 $LAST_STEP || exit 1

    # Prompt user before teardown
    echo ""
    echo "=== Cluster is ready for inspection ==="
    echo "The cluster is now running and ready to inspect."
    echo "Work directory: $TEST_WORKDIR"
    echo "You can:"
    echo "  - Run: kubectl get nodes"
    echo "  - Run: kubectl get pods -A"
    echo "  - SSH to nodes using the aliases from env.sh"
    echo "  - Test Cassandra connectivity"
    echo ""
    read -p "Press Enter to tear down the cluster and complete the test..."
    echo ""

    # Now run teardown
    step_teardown
    EXIT_OK=true
else
    # Run all steps
    run_from_step 0
    EXIT_OK=true
fi
