#!/bin/bash
#
# End-to-End Test Runner for easy-db-lab
#
# This script provisions a full cluster environment and runs a sequence of
# validation steps (build, deploy, configure, test, teardown). It is designed
# for resilience: a single step failure does NOT stop the run.
#
# Failure Handling:
#   - Every step runs regardless of whether earlier steps failed.
#   - When a step fails, diagnostics are captured (step output, pod status,
#     K8s events, disk usage) and appended to a temporary failure log.
#   - A running FAILURE_COUNT tracks how many steps have failed.
#
# Teardown Behavior:
#   - If all steps pass, the script exits cleanly (exit 0).
#   - If any step failed, the full failure log is printed at the end and the
#     script prompts for explicit confirmation ("yes") before tearing down
#     the cluster. This keeps the environment alive for debugging. Press
#     Ctrl-C to exit without teardown.
#
# Flags:
#   --list-steps, -l   List all steps and exit.
#   --break N[,M,...]  Pause before the given step number(s) for inspection.
#   --build            Build the packer AMI image before running.
#   --spark            Enable Spark-related steps.
#   --cassandra        Enable Cassandra-related steps.
#   --clickhouse       Enable ClickHouse-related steps.
#   --opensearch       Enable OpenSearch-related steps.
#   --ebs              Use EBS volumes.
#

set -e  # Exit on any error

# Store project root directory
PROJECT_ROOT="$(pwd)"

# Work directory is the same as project root (no temp directory)
TEST_WORKDIR="$PROJECT_ROOT"

# Prepend bin directory to PATH so easy-db-lab uses this worktree's version
export PATH="$PROJECT_ROOT/bin:$PATH"

# Use sandbox-admin AWS profile for S3 operations
export AWS_PROFILE=sandbox-admin

# Track current step for resume functionality
CURRENT_STEP=0
CURRENT_STEP_NAME=""
STEP_LOG=$(mktemp /tmp/e2e-step-XXXXXX)
# mktemp requires XXXXXX at the very end of the template — no suffix allowed
FAILURE_LOG=$(mktemp /tmp/e2e-failures-XXXXXX)
FAILURE_COUNT=0
SPARK_JOB_RAN=false

# Parse command line arguments (must be before cleanup trap setup)
BUILD_IMAGE=false
ENABLE_SPARK=false
ENABLE_CASSANDRA=false
ENABLE_CLICKHOUSE=false
ENABLE_OPENSEARCH=false
USE_EBS=false
EXIT_OK=false
LIST_STEPS=false
BREAKPOINTS=""
START_STEP=""
while [[ $# -gt 0 ]]; do
  case $1 in
    --list-steps|-l)
      LIST_STEPS=true
      shift
      ;;
    --break)
      BREAKPOINTS="$2"
      shift 2
      ;;
    --build)
      BUILD_IMAGE=true
      shift
      ;;
    --spark)
      ENABLE_SPARK=true
      ENABLE_CASSANDRA=true
      shift
      ;;
    --cassandra)
      ENABLE_CASSANDRA=true
      shift
      ;;
    --clickhouse)
      ENABLE_CLICKHOUSE=true
      shift
      ;;
    --opensearch)
      ENABLE_OPENSEARCH=true
      shift
      ;;
    --all)
      ENABLE_SPARK=true
      ENABLE_CASSANDRA=true
      ENABLE_CLICKHOUSE=true
      ENABLE_OPENSEARCH=true
      shift
      ;;
    --ebs)
      USE_EBS=true
      shift
      ;;
    --start-step)
      START_STEP="$2"
      shift 2
      ;;
    *)
      echo "Unknown option: $1"
      echo "Usage: $0 [--list-steps|-l] [--break <steps>] [--start-step <N>] [--build] [--spark] [--cassandra] [--clickhouse] [--opensearch] [--all] [--ebs]"
      exit 1
      ;;
  esac
done

# ============================================================================
# Test Step Functions
# ============================================================================

step_build_project() {
    echo "=== Building project ==="
    if [ "$ENABLE_SPARK" = true ]; then
        # Pre-build cassandra-analytics with JDK 11 for bulk-writer
        echo "=== Pre-building Cassandra Analytics (JDK 11) ==="
        "$PROJECT_ROOT/bin/build-cassandra-analytics"
        (cd "$PROJECT_ROOT" && ./gradlew clean shadowJar installDist :spark-connector-test1:jar :bulk-writer:jar)
    else
        (cd "$PROJECT_ROOT" && ./gradlew clean shadowJar installDist)
    fi
}

step_check_version() {
    echo "=== Checking version ==="
    easy-db-lab version
}

step_build_image() {
    if [ "$BUILD_IMAGE" = true ]; then
        echo "=== Building packer images ==="
        easy-db-lab build-image
    else
        echo "=== Skipping packer image build (use --build to enable) ==="
    fi
}

step_set_policies() {
    echo "=== Creating IAM managed policies ==="
    "$PROJECT_ROOT/bin/set-policies" --group-name EasyDBLabUsers --profile sandbox-admin
}

step_init_cluster() {
    echo "=== Initializing cluster ==="
    local spark_opts=""
    if [ "$ENABLE_SPARK" = true ]; then
        spark_opts="--spark.enable --spark.master.instance.type m5.xlarge --spark.worker.instance.type m5.xlarge --spark.worker.instance.count 2"
        echo "=== Spark provisioning enabled ==="
    fi
    local ebs_opts=""
    if [ "$USE_EBS" = true ]; then
        ebs_opts="--ebs.type gp3 --ebs.size 256"
        echo "=== EBS volumes enabled (gp3, 256GB) ==="
    fi
    local created_timestamp=$(date +%s)
    easy-db-lab init -c 3 -i c5.2xlarge test --clean --up -s 1 --tag "created=$created_timestamp" --cidr 10.14.0.0/20 $spark_opts $ebs_opts
}

step_setup_kubectl() {
    echo "=== Setting up kubectl access ==="
    source env.sh
}

step_wait_k3s_ready() {
    echo "=== Checking K3s cluster connectivity ==="
    if kubectl get nodes &>/dev/null; then
        echo "kubectl connectivity established"
        return 0
    fi
    echo "ERROR: kubectl failed to connect to K3s cluster"
    return 1
}

step_verify_k3s() {
    echo "=== Verifying K3s cluster nodes ==="
    kubectl get nodes

    echo "=== Verifying K3s system pods ==="
    kubectl get pods -A

    echo "=== Waiting for all nodes to be Ready ==="
    kubectl wait --for=condition=Ready nodes --all --timeout=120s
}

step_verify_vpc_tags() {
    echo "=== Verifying VPC tags ==="

    # Extract VPC ID from state.json
    local vpc_id=$(jq -r '.vpcId' state.json)
    if [ -z "$vpc_id" ] || [ "$vpc_id" = "null" ]; then
        echo "ERROR: VPC ID not found in state.json"
        return 1
    fi

    echo "VPC ID: $vpc_id"

    # Get VPC tags using AWS CLI
    local tags=$(aws ec2 describe-vpcs --vpc-ids "$vpc_id" --query 'Vpcs[0].Tags' --output json)
    echo "VPC Tags: $tags"

    # Verify easy_cass_lab system tag is present
    local easy_cass_lab_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "easy_cass_lab") | .Value')
    if [ "$easy_cass_lab_tag" = "1" ]; then
        echo "  easy_cass_lab tag: OK (value: $easy_cass_lab_tag)"
    else
        echo "ERROR: easy_cass_lab tag not found or incorrect"
        return 1
    fi

    # Verify ClusterId system tag is present
    local cluster_id_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "ClusterId") | .Value')
    if [ -n "$cluster_id_tag" ] && [ "$cluster_id_tag" != "null" ]; then
        echo "  ClusterId tag: OK (value: $cluster_id_tag)"
    else
        echo "ERROR: ClusterId tag not found"
        return 1
    fi

    # Verify user-supplied 'created' tag is present (from init --tag "created=$timestamp")
    local created_tag=$(echo "$tags" | jq -r '.[] | select(.Key == "created") | .Value')
    if [ -n "$created_tag" ] && [ "$created_tag" != "null" ]; then
        echo "  created tag: OK (value: $created_tag)"
    else
        echo "ERROR: User-supplied 'created' tag not found on VPC"
        echo "This tag was specified via 'init --tag created=...' and should be propagated to the VPC"
        return 1
    fi

    echo "=== VPC tag verification passed ==="
}

step_list_hosts() {
    echo "=== Listing cluster hosts ==="
    easy-db-lab hosts

    echo "=== Status Without Cassandra Version ==="
    easy-db-lab status

    echo "=== Listing available Cassandra versions ==="
    easy-db-lab cassandra list
}

step_test_mcp_server() {
    echo "=== Testing MCP server /status endpoint ==="

    # Remove stale .mcp.json if present
    rm -f .mcp.json

    # Kill any stale server on this port
    local server_port=9852
    local stale_pid=$(lsof -ti :$server_port 2>/dev/null || true)
    if [ -n "$stale_pid" ]; then
        echo "Killing stale process on port $server_port (PID: $stale_pid)"
        kill "$stale_pid" 2>/dev/null || true
        sleep 2
    fi

    # Start the MCP server in the background on a fixed port.
    # Redirect output to a log file so the server process doesn't hold
    # the tee pipe open after this step returns.
    local server_log=$(mktemp /tmp/e2e-mcp-server-XXXXXX)
    easy-db-lab server --port $server_port > "$server_log" 2>&1 &
    local server_pid=$!
    echo "$server_pid" > .mcp-server.pid
    local server_url="http://127.0.0.1:${server_port}"

    # Wait for the server to accept connections
    local timeout=120
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
        if curl -s -o /dev/null "${server_url}/status" 2>/dev/null; then
            echo "MCP server ready at: $server_url"
            break
        fi
        sleep 2
        elapsed=$((elapsed + 2))
    done

    if [ $elapsed -ge $timeout ]; then
        echo "ERROR: MCP server did not become ready within ${timeout}s"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Call /status endpoint
    echo "=== Calling /status endpoint ==="
    local http_code
    local response
    response=$(curl -s -w "\n%{http_code}" "${server_url}/status")
    http_code=$(echo "$response" | tail -1)
    local body
    body=$(echo "$response" | sed '$d')

    echo "HTTP status code: $http_code"
    echo "Response body (first 20 lines):"
    echo "$body" | head -20

    # Verify HTTP 200
    if [ "$http_code" != "200" ]; then
        echo "ERROR: Expected HTTP 200, got $http_code"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify response contains cluster data
    if echo "$body" | jq -e '.cluster' > /dev/null 2>&1; then
        echo "Response contains cluster data: OK"
    else
        echo "ERROR: Response does not contain expected cluster data"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify response contains nodes data
    if echo "$body" | jq -e '.nodes' > /dev/null 2>&1; then
        echo "Response contains nodes data: OK"
    else
        echo "ERROR: Response does not contain expected nodes data"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Verify S3 section contains fullpath field and save for later steps
    if echo "$body" | jq -e '.s3.fullpath' > /dev/null 2>&1; then
        local fullpath=$(echo "$body" | jq -r '.s3.fullpath')
        echo "$fullpath" > .s3-fullpath
        echo "Response contains s3.fullpath: OK ($fullpath)"
    else
        echo "ERROR: Response does not contain expected s3.fullpath field"
        kill "$server_pid" 2>/dev/null || true
        return 1
    fi

    # Clean up — kill process group to catch any children
    echo "=== Stopping MCP server ==="
    kill -9 "$server_pid" 2>/dev/null || true
    wait "$server_pid" 2>/dev/null || true
    rm -f .mcp.json .mcp-server.pid "$server_log"

    echo "=== MCP server /status test passed ==="
}

step_verify_s3_backup() {
    echo "=== Verifying S3 config backup ==="

    local s3_fullpath=$(cat .s3-fullpath)
    local s3_bucket=$(jq -r '.s3Bucket' state.json)

    echo "S3 Fullpath: $s3_fullpath"

    # Verify key backup files exist
    local missing_files=()

    echo "=== Checking kubeconfig backup ==="
    if aws s3 ls "s3://${s3_fullpath}/config/kubeconfig" > /dev/null 2>&1; then
        echo "  config/kubeconfig: OK"
    else
        echo "  config/kubeconfig: MISSING"
        missing_files+=("config/kubeconfig")
    fi

    echo "=== Checking config backups ==="
    for config_file in "config/cassandra_versions.yaml" "config/environment.sh" "config/setup_instance.sh"; do
        if aws s3 ls "s3://${s3_fullpath}/${config_file}" > /dev/null 2>&1; then
            echo "  ${config_file}: OK"
        else
            echo "  ${config_file}: MISSING"
            missing_files+=("${config_file}")
        fi
    done

    echo "=== Listing all S3 backup contents ==="
    aws s3 ls "s3://${s3_bucket}/" --recursive | head -30

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing backup files in s3://${s3_fullpath}/:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== S3 backup verification passed ==="
}

step_setup_cassandra() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra setup (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Setting Cassandra version to 5.0 ==="
    easy-db-lab cassandra use 5.0

    echo "=== Updating configuration ==="
    easy-db-lab cassandra update-config

    echo "=== Starting Cassandra ==="
    easy-db-lab cassandra start
}

step_verify_cassandra_backup() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra backup verification (use --cassandra to enable) ==="
        return 0
    fi

    echo "=== Verifying Cassandra config backup to S3 ==="

    local s3_fullpath=$(cat .s3-fullpath)

    local missing_files=()

    echo "=== Checking cassandra.patch.yaml backup ==="
    if aws s3 ls "s3://${s3_fullpath}/config/cassandra.patch.yaml" > /dev/null 2>&1; then
        echo "  config/cassandra.patch.yaml: OK"
    else
        echo "  config/cassandra.patch.yaml: MISSING"
        missing_files+=("config/cassandra.patch.yaml")
    fi

    echo "=== Checking cassandra config directory backup ==="
    local cassandra_config_count=$(aws s3 ls "s3://${s3_fullpath}/config/cassandra-config/" --recursive 2>/dev/null | wc -l)
    if [ "$cassandra_config_count" -gt 0 ]; then
        echo "  config/cassandra-config: OK ($cassandra_config_count files)"
    else
        echo "  config/cassandra-config: MISSING"
        missing_files+=("config/cassandra-config/")
    fi

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing Cassandra backup files in s3://${s3_fullpath}/:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== Cassandra backup verification passed ==="
}

step_verify_restore() {
    echo "=== Verifying restore from VPC ID ==="

    # Extract VPC ID from state.json
    local vpc_id=$(jq -r '.vpcId' state.json)
    if [ -z "$vpc_id" ] || [ "$vpc_id" = "null" ]; then
        echo "ERROR: VPC ID not found in state.json"
        return 1
    fi

    echo "VPC ID: $vpc_id"

    # Create temporary restore directory
    local restore_dir=$(mktemp -d)
    echo "=== Using temporary restore directory: $restore_dir ==="

    # Change into restore directory and run status with EASY_DB_LAB_RESTORE_VPC env var
    echo "=== Running status with EASY_DB_LAB_RESTORE_VPC to trigger restore ==="
    (
        cd "$restore_dir"
        EASY_DB_LAB_RESTORE_VPC="$vpc_id" easy-db-lab status
    )

    # Verify restored files
    local missing_files=()

    echo "=== Verifying restored files ==="

    # Check state.json was created
    if [ -f "$restore_dir/state.json" ]; then
        echo "  state.json: OK"
    else
        echo "  state.json: MISSING"
        missing_files+=("state.json")
    fi

    # Check kubeconfig was restored
    if [ -f "$restore_dir/kubeconfig" ]; then
        echo "  kubeconfig: OK"
    else
        echo "  kubeconfig: MISSING"
        missing_files+=("kubeconfig")
    fi

    # Check environment.sh was restored
    if [ -f "$restore_dir/environment.sh" ]; then
        echo "  environment.sh: OK"
    else
        echo "  environment.sh: MISSING"
        missing_files+=("environment.sh")
    fi

    # Check setup_instance.sh was restored
    if [ -f "$restore_dir/setup_instance.sh" ]; then
        echo "  setup_instance.sh: OK"
    else
        echo "  setup_instance.sh: MISSING"
        missing_files+=("setup_instance.sh")
    fi

    # Check cassandra_versions.yaml was restored
    if [ -f "$restore_dir/cassandra_versions.yaml" ]; then
        echo "  cassandra_versions.yaml: OK"
    else
        echo "  cassandra_versions.yaml: MISSING"
        missing_files+=("cassandra_versions.yaml")
    fi

    # Verify Cassandra-specific files if Cassandra was enabled
    if [ "$ENABLE_CASSANDRA" = true ]; then
        if [ -f "$restore_dir/cassandra.patch.yaml" ]; then
            echo "  cassandra.patch.yaml: OK"
        else
            echo "  cassandra.patch.yaml: MISSING"
            missing_files+=("cassandra.patch.yaml")
        fi

        local cassandra_config_count=$(find "$restore_dir/cassandra" -type f 2>/dev/null | wc -l)
        if [ "$cassandra_config_count" -gt 0 ]; then
            echo "  cassandra/: OK ($cassandra_config_count files)"
        else
            echo "  cassandra/: MISSING or empty"
            missing_files+=("cassandra/")
        fi
    fi

    if [ ${#missing_files[@]} -ne 0 ]; then
        echo "ERROR: Missing restored files in $restore_dir:"
        for f in "${missing_files[@]}"; do
            echo "  - $f"
        done
        return 1
    fi

    echo "=== Restore verification passed ==="
}

step_cassandra_start_stop() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Cassandra start/stop cycle (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Starting Cassandra ==="
    easy-db-lab cassandra start

    echo "=== Status With Cassandra Running ==="
    easy-db-lab status

    echo "=== Stopping Cassandra ==="
    easy-db-lab cassandra stop

    echo "=== Waiting for Cassandra to stop ==="
    sleep 10

    echo "=== Starting Cassandra again ==="
    easy-db-lab cassandra start

    echo "=== Restarting Cassandra ==="
    easy-db-lab cassandra restart
}

step_test_ssh_nodetool() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping SSH nodetool test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Testing SSH access and nodetool via env.sh aliases ==="
    ssh db0 nodetool status
}

step_check_sidecar() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping Sidecar check (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Check Sidecar ==="
    ssh db0 "curl -s http://$(easy-db-lab ip db0 --private):9043/api/v1/cassandra/schema" | jq 'keys'
}

step_test_exec() {
    echo "=== Testing exec command (sequential) ==="
    easy-db-lab exec -t cassandra hostname

    echo "=== Testing exec command (parallel) ==="
    easy-db-lab exec -t cassandra -p uptime

    echo "=== Testing exec command (with host filter) ==="
    easy-db-lab exec -t cassandra --hosts db0,db1 date
}

step_stress_test() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping stress test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Running stress test ==="
    ssh app0 "bash -l -c 'cassandra-easy-stress run KeyValue -d 10s'"
}

step_stress_k8s() {
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping stress K8s test (use --cassandra to enable) ==="
        return 0
    fi
    echo "=== Starting stress job on K8s ==="
    easy-db-lab cassandra stress run --name e2e-test -- KeyValue -d 30s

    echo "=== Checking stress job status ==="
    easy-db-lab cassandra stress status

    echo "=== Waiting for stress job to complete ==="
    local timeout=120
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
        local status=$(kubectl get jobs -l app.kubernetes.io/name=cassandra-easy-stress -o jsonpath='{.items[0].status.succeeded}' 2>/dev/null || echo "")
        if [ "$status" = "1" ]; then
            echo "Stress job completed successfully"
            break
        fi
        echo "Waiting for stress job to complete... ($elapsed/$timeout seconds)"
        sleep 10
        elapsed=$((elapsed + 10))
    done

    echo "=== Viewing stress job logs ==="
    # Get the job name that we just created (most recent)
    local job_name=$(kubectl get jobs -l app.kubernetes.io/name=cassandra-easy-stress --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1].metadata.name}')
    if [ -n "$job_name" ]; then
        easy-db-lab cassandra stress logs "$job_name" --tail 50
    fi

    echo "=== Stopping stress job ==="
    easy-db-lab cassandra stress stop --all

    echo "=== Verifying stress jobs are deleted ==="
    easy-db-lab cassandra stress status
}

step_spark_submit() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping Spark job submission (use --spark to enable) ==="
        return 0
    fi
    echo "=== Submitting Spark job to EMR ==="
    easy-db-lab spark submit \
      --jar "$PROJECT_ROOT/spark-connector-test1/build/libs/spark-connector-test1-12.jar" \
      --main-class com.rustyrazorblade.easydblab.spark.KeyValuePrefixCount \
      --args "$(easy-db-lab ip db0 --private)" \
      --wait
    SPARK_JOB_RAN=true
}

step_spark_status() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping Spark status check (use --spark to enable) ==="
        return 0
    fi
    echo "=== Listing Spark jobs ==="
    easy-db-lab spark jobs

    echo "=== Checking Spark job status (automatically downloads logs) ==="
    local status_output
    status_output=$(easy-db-lab spark status 2>&1)
    echo "$status_output"
    if echo "$status_output" | grep -q "State: FAILED"; then
        echo "ERROR: Spark job is in FAILED state"

        echo ""
        echo "=== EMR Failure Diagnostics ==="

        local emr_cluster_id step_id bucket cluster_prefix
        emr_cluster_id=$(jq -r '.emrCluster.clusterId // empty' state.json 2>/dev/null)
        if [ -n "$emr_cluster_id" ]; then
            step_id=$(aws emr list-steps --cluster-id "$emr_cluster_id" \
                --query 'Steps[0].Id' --output text 2>/dev/null || true)

            echo "--- aws emr describe-step ---"
            aws emr describe-step --cluster-id "$emr_cluster_id" \
                --step-id "$step_id" --output json 2>/dev/null || echo "(describe-step failed)"

            echo ""
            echo "--- EMR Step Stderr (last 100 lines) ---"
            bucket=$(jq -r '.s3Bucket // empty' state.json 2>/dev/null)
            cluster_prefix=$(jq -r '"clusters/\(.name)-\(.clusterId)"' state.json 2>/dev/null)
            if [ -n "$bucket" ] && [ -n "$step_id" ]; then
                local log_base="s3://$bucket/$cluster_prefix/spark/emr-logs/$emr_cluster_id/steps/$step_id"
                aws s3 cp "$log_base/stderr.gz" - 2>/dev/null \
                    | gunzip 2>/dev/null | tail -100 \
                    || echo "(stderr not available in S3)"

                echo ""
                echo "--- EMR Step Stdout (last 50 lines) ---"
                aws s3 cp "$log_base/stdout.gz" - 2>/dev/null \
                    | gunzip 2>/dev/null | tail -50 \
                    || echo "(stdout not available in S3)"

                echo ""
                echo "--- S3 log files available ---"
                aws s3 ls "$log_base/" 2>/dev/null \
                    || echo "(S3 listing failed)"
            else
                echo "(bucket or step_id not available)"
            fi
        else
            echo "(EMR cluster ID not found in state.json)"
        fi

        return 1
    fi

    echo "=== Downloading all EMR logs (standalone command) ==="
    easy-db-lab spark logs

    if [ "$SPARK_JOB_RAN" = true ]; then
        echo "=== Verifying Spark logs reached VictoriaLogs via OTel ==="
        local spark_logs=$(ssh control0 "curl -s 'http://localhost:9428/select/logsql/query?query=service.name%3A~%22spark-.*%22&limit=5'")
        local spark_log_count=$(echo "$spark_logs" | grep -c '^{' || true)
        if [ "$spark_log_count" -gt 0 ]; then
            echo "Spark OTel logs found in VictoriaLogs: OK ($spark_log_count entries)"
        else
            echo "ERROR: No Spark logs found in VictoriaLogs with service.name:~\"spark-*\""
            echo "OTel Java agent may not be exporting logs to the control node"
            return 1
        fi
    else
        echo "=== Skipping Spark VictoriaLogs check (no Spark job was submitted successfully) ==="
    fi
}

step_bulk_writer_direct() {
    if [ "$ENABLE_SPARK" != true ]; then
        echo "=== Skipping bulk writer test (use --spark to enable) ==="
        return 0
    fi
    if [ "$ENABLE_CASSANDRA" != true ]; then
        echo "=== Skipping bulk writer test (use --cassandra to enable for table creation) ==="
        return 0
    fi

    echo "=== Creating bulk writer test table ==="
    easy-db-lab cassandra cql "CREATE KEYSPACE IF NOT EXISTS bulk_test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"
    easy-db-lab cassandra cql "CREATE TABLE IF NOT EXISTS bulk_test.data (id bigint PRIMARY KEY, course blob, marks bigint)"

    echo "=== Submitting bulk writer (direct mode) to EMR ==="
    local sidecar_hosts=$(easy-db-lab hosts --type cassandra --private | tr '\n' ',' | sed 's/,$//')

    easy-db-lab spark submit \
      --jar "$PROJECT_ROOT/bulk-writer/build/libs/bulk-writer.jar" \
      --main-class com.rustyrazorblade.easydblab.spark.DirectBulkWriter \
      --args "$sidecar_hosts" bulk_test data datacenter1 10000 4 \
      --wait

    echo "=== Verifying bulk write results ==="
    easy-db-lab cassandra cql "SELECT COUNT(*) FROM bulk_test.data"
}

step_clickhouse_start() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse deployment (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Initializing ClickHouse configuration ==="
    easy-db-lab clickhouse init

    echo "=== Deploying ClickHouse cluster ==="
    easy-db-lab clickhouse start

    echo "=== Checking ClickHouse cluster status ==="
    easy-db-lab clickhouse status

    echo "=== Waiting for ClickHouse pods to be ready ==="
    kubectl wait --for=condition=Ready pods --all -n clickhouse --timeout=300s

    echo "=== Verifying ClickHouse pods ==="
    kubectl get pods -n clickhouse
}

step_clickhouse_test() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse test (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Testing ClickHouse connectivity ==="

    clickhouse-query "SELECT version()"

    clickhouse-query <<'EOF'
CREATE OR REPLACE TABLE test (
    id UInt64,
    updated_at DateTime DEFAULT now(),
    updated_at_date Date DEFAULT toDate(updated_at)
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/test', '{replica}')
ORDER BY id
SETTINGS storage_policy = 's3_main'
EOF

    clickhouse-query "INSERT INTO test (id) VALUES (1)"
}

step_clickhouse_stop() {
    if [ "$ENABLE_CLICKHOUSE" != true ]; then
        echo "=== Skipping ClickHouse stop (use --clickhouse to enable) ==="
        return 0
    fi
    echo "=== Stopping ClickHouse cluster ==="
    easy-db-lab clickhouse stop --force

    echo "=== Verifying ClickHouse namespace is deleted ==="
    local timeout=60
    local elapsed=0
    while [ $elapsed -lt $timeout ]; do
      if ! kubectl get namespace clickhouse &>/dev/null; then
        echo "ClickHouse namespace successfully deleted"
        return 0
      fi
      echo "Waiting for ClickHouse namespace to be deleted... ($elapsed/$timeout seconds)"
      sleep 5
      elapsed=$((elapsed + 5))
    done
}

step_opensearch_start() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch deployment (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Deploying OpenSearch domain (this takes 10-30 minutes) ==="
    easy-db-lab opensearch start --wait

    echo "=== Checking OpenSearch domain status ==="

    # should be ready
    easy-db-lab opensearch status
}

step_opensearch_test() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch test (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Testing OpenSearch connectivity ==="

    # Get endpoint using --endpoint flag
    local endpoint=$(easy-db-lab opensearch status --endpoint)
    if [ -z "$endpoint" ]; then
        echo "ERROR: Could not get OpenSearch endpoint"
        return 1
    fi

    echo "=== OpenSearch endpoint: $endpoint ==="

    # Test cluster health via control node
    echo "=== Testing cluster health ==="
    ssh control0 "curl -s -k https://${endpoint}/_cluster/health" | jq .

    # Create test index
    echo "=== Creating test index ==="
    ssh control0 "curl -s -k -X PUT https://${endpoint}/test-index"

    # Insert test document
    echo "=== Inserting test document ==="
    ssh control0 "curl -s -k -X POST https://${endpoint}/test-index/_doc/1 -H 'Content-Type: application/json' -d '{\"message\": \"hello from e2e test\"}'"

    # Verify document
    echo "=== Verifying test document ==="
    ssh control0 "curl -s -k https://${endpoint}/test-index/_doc/1" | jq .
}

step_opensearch_stop() {
    if [ "$ENABLE_OPENSEARCH" != true ]; then
        echo "=== Skipping OpenSearch stop (use --opensearch to enable) ==="
        return 0
    fi
    echo "=== Stopping OpenSearch domain ==="
    easy-db-lab opensearch stop --force
}

step_test_observability() {
    echo "=== Testing VictoriaMetrics and VictoriaLogs ==="

    # Wait for VictoriaMetrics to be ready
    echo "=== Waiting for VictoriaMetrics pod to be ready ==="
    kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=victoriametrics --timeout=120s

    # Wait for VictoriaLogs to be ready
    echo "=== Waiting for VictoriaLogs pod to be ready ==="
    kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=victorialogs --timeout=120s

    # Test VictoriaMetrics health endpoint
    echo "=== Testing VictoriaMetrics health ==="
    local vm_health=$(ssh control0 "curl -s http://localhost:8428/health")
    echo "VictoriaMetrics health: $vm_health"

    # Test VictoriaLogs health endpoint
    echo "=== Testing VictoriaLogs health ==="
    local vl_health=$(ssh control0 "curl -s http://localhost:9428/health")
    echo "VictoriaLogs health: $vl_health"

    # Query VictoriaMetrics for metrics (should have some metrics from OTel)
    echo "=== Querying VictoriaMetrics for metrics ==="
    local metrics_count=$(ssh control0 "curl -s 'http://localhost:8428/api/v1/label/__name__/values' | jq '.data | length'")
    echo "Number of metric names in VictoriaMetrics: $metrics_count"
    if [ "$metrics_count" -lt 1 ]; then
        echo "WARNING: No metrics found in VictoriaMetrics yet (may need more time for OTel to send data)"
    fi

    # Check YACE CloudWatch metrics if Spark/EMR is enabled
    if [ "$ENABLE_SPARK" = true ]; then
        echo "=== Checking YACE CloudWatch metrics for S3 ==="
        local s3_metrics=$(ssh control0 "curl -s 'http://localhost:8428/api/v1/label/__name__/values' | jq '[.data[] | select(startswith(\"aws_s3\"))] | length'")
        echo "S3 CloudWatch metric names via YACE: $s3_metrics"
        if [ "$s3_metrics" -lt 1 ]; then
            echo "WARNING: No aws_s3_* metrics found yet - YACE scrapes CloudWatch every 5 minutes"
        else
            echo "YACE S3 metrics: OK"
        fi
    fi

    # Query VictoriaLogs to check it's accepting connections
    echo "=== Testing VictoriaLogs query endpoint ==="
    local vl_query=$(ssh control0 "curl -s -o /dev/null -w '%{http_code}' 'http://localhost:9428/select/logsql/query?query=*'")
    echo "VictoriaLogs query endpoint response code: $vl_query"
    if [ "$vl_query" != "200" ]; then
        echo "WARNING: VictoriaLogs query endpoint returned unexpected status: $vl_query"
    fi

    # Test Grafana is up and datasources are configured
    echo "=== Testing Grafana health ==="
    local grafana_health=$(ssh control0 "curl -s http://localhost:3000/api/health | jq -r '.database'")
    echo "Grafana database status: $grafana_health"

    echo "=== Testing Grafana datasources ==="
    local datasources=$(ssh control0 "curl -s http://localhost:3000/api/datasources | jq -r '.[].name'")
    echo "Configured datasources:"
    echo "$datasources"

    # Verify VictoriaMetrics datasource exists
    if echo "$datasources" | grep -q "VictoriaMetrics"; then
        echo "VictoriaMetrics datasource: OK"
    else
        echo "ERROR: VictoriaMetrics datasource not found!"
        return 1
    fi

    # Verify VictoriaLogs datasource exists
    if echo "$datasources" | grep -q "VictoriaLogs"; then
        echo "VictoriaLogs datasource: OK"
    else
        echo "ERROR: VictoriaLogs datasource not found!"
        return 1
    fi

    echo "=== Observability stack test completed ==="
}

step_test_dashboards() {
    echo "=== Validating Grafana dashboards ==="

    local dashboards=$(ssh control0 "curl -s 'http://localhost:3000/api/search?type=dash-db' | jq -r '.[].title'")
    echo "Deployed dashboards:"
    echo "$dashboards"

    local dashboard_count=$(echo "$dashboards" | wc -l)
    if [ "$dashboard_count" -lt 1 ]; then
        echo "ERROR: No dashboards found in Grafana"
        return 1
    fi
    echo "Dashboard count: $dashboard_count"

    # For each dashboard, hit the API to check it loads without errors
    local uids=$(ssh control0 "curl -s 'http://localhost:3000/api/search?type=dash-db' | jq -r '.[].uid'")
    local failures=0
    for uid in $uids; do
        local status=$(ssh control0 "curl -s -o /dev/null -w '%{http_code}' 'http://localhost:3000/api/dashboards/uid/$uid'")
        local title=$(ssh control0 "curl -s 'http://localhost:3000/api/dashboards/uid/$uid' | jq -r '.dashboard.title'")
        if [ "$status" = "200" ]; then
            echo "  $title: OK"
        else
            echo "  $title: FAILED (HTTP $status)"
            failures=$((failures + 1))
        fi
    done

    if [ "$failures" -gt 0 ]; then
        echo "ERROR: $failures dashboard(s) failed to load"
        return 1
    fi
    echo "=== Dashboard validation completed ==="
}

step_test_logs_query() {
    echo "=== Testing logs query command ==="

    # Query logs using the CLI - this tests the full pipeline:
    # OTel Collector -> VictoriaLogs -> VictoriaLogsService -> CLI output
    echo "=== Querying all logs (should have systemd/journald logs) ==="
    local log_output=$(easy-db-lab logs query --since 1h --limit 10 2>&1) || true
    echo "$log_output"

    # Check if we got any results (not just error messages)
    if echo "$log_output" | grep -q "Found .* log entries"; then
        echo "=== Logs query returned results: OK ==="
    elif echo "$log_output" | grep -q "No logs found"; then
        echo "WARNING: No logs found yet - OTel Collector may need more time to ingest"
    elif echo "$log_output" | grep -q "Failed to query logs"; then
        echo "ERROR: Logs query failed"
        echo "$log_output"
        return 1
    fi

    # If a Spark job ran successfully, check for EMR logs
    if [ "$SPARK_JOB_RAN" = true ]; then
        echo "=== Querying EMR logs ==="
        easy-db-lab logs query --source emr --since 1h --limit 5 || true
    fi

    echo "=== Logs query test completed ==="
}

step_test_metrics_backup() {
    echo "=== Testing VictoriaMetrics backup ==="

    # Run metrics backup command
    easy-db-lab metrics backup

    local s3_fullpath=$(cat .s3-fullpath)

    # Verify backup was created in S3
    echo "=== Checking VictoriaMetrics backup in S3 ==="
    local backup_count=$(aws s3 ls "s3://${s3_fullpath}/victoriametrics/" 2>/dev/null | wc -l)
    if [ "$backup_count" -gt 0 ]; then
        echo "  victoriametrics backup: OK ($backup_count entries)"
        aws s3 ls "s3://${s3_fullpath}/victoriametrics/" | head -5
    else
        echo "ERROR: VictoriaMetrics backup not found in S3"
        echo "Contents of s3://${s3_fullpath}/:"
        aws s3 ls "s3://${s3_fullpath}/" 2>/dev/null || echo "(listing failed)"
        return 1
    fi

    echo "=== VictoriaMetrics backup test completed ==="
}

step_test_logs_backup() {
    echo "=== Testing VictoriaLogs backup ==="

    # Run logs backup command
    easy-db-lab logs backup

    local s3_fullpath=$(cat .s3-fullpath)

    # Verify backup was created in S3
    echo "=== Checking VictoriaLogs backup in S3 ==="
    local backup_count=$(aws s3 ls "s3://${s3_fullpath}/victorialogs/" 2>/dev/null | wc -l)
    if [ "$backup_count" -gt 0 ]; then
        echo "  victorialogs backup: OK ($backup_count entries)"
        aws s3 ls "s3://${s3_fullpath}/victorialogs/" | head -5
    else
        echo "ERROR: VictoriaLogs backup not found in S3"
        echo "Contents of s3://${s3_fullpath}/:"
        aws s3 ls "s3://${s3_fullpath}/" 2>/dev/null || echo "(listing failed)"
        return 1
    fi

    echo "=== VictoriaLogs backup test completed ==="
}

step_teardown() {
    # Kill stale MCP server if still running
    if [ -f .mcp-server.pid ]; then
        local pid=$(cat .mcp-server.pid)
        kill "$pid" 2>/dev/null || true
        rm -f .mcp-server.pid
    fi
    rm -f .s3-fullpath
    rm -f "$STEP_LOG"

    echo "=== Tearing down cluster ==="
    easy-db-lab down --yes
    echo "=== Teardown complete ==="
}

# ============================================================================
# Step Registry and Runner
# ============================================================================

# Steps that run from project root (before cd to TEST_WORKDIR)
STEPS_PROJECT_ROOT=(
    "step_build_project:Build project"
    "step_check_version:Check version"
    "step_build_image:Build packer image"
)

# Steps that run from TEST_WORKDIR
STEPS_WORKDIR=(
    "step_set_policies:Set IAM policies"
    "step_init_cluster:Initialize cluster"
    "step_setup_kubectl:Setup kubectl"
    "step_wait_k3s_ready:Wait for K3s"
    "step_verify_k3s:Verify K3s cluster"
    "step_verify_vpc_tags:Verify VPC tags"
    "step_list_hosts:List hosts"
    "step_test_mcp_server:Test MCP server status endpoint"
    "step_verify_s3_backup:Verify S3 backup"
    "step_setup_cassandra:Setup Cassandra"
    "step_verify_cassandra_backup:Verify Cassandra backup"
    "step_verify_restore:Verify restore from VPC"
    "step_test_ssh_nodetool:Test SSH and nodetool"
    "step_check_sidecar:Check Sidecar"
    "step_test_exec:Test exec command"
    "step_stress_test:Run stress test"
    "step_stress_k8s:Run stress K8s test"
    "step_spark_submit:Submit Spark job"
    "step_spark_status:Check Spark status"
    "step_bulk_writer_direct:Test bulk writer (direct)"
    "step_cassandra_start_stop:Cassandra start/stop cycle"
    "step_clickhouse_start:Start ClickHouse"
    "step_clickhouse_test:Test ClickHouse"
    "step_clickhouse_stop:Stop ClickHouse"
    "step_opensearch_start:Start OpenSearch"
    "step_opensearch_test:Test OpenSearch"
    "step_opensearch_stop:Stop OpenSearch"
    "step_test_observability:Test observability stack"
    "step_test_dashboards:Validate Grafana dashboards"
    "step_test_logs_query:Test logs query command"
    "step_test_metrics_backup:Test VictoriaMetrics backup"
    "step_test_logs_backup:Test VictoriaLogs backup"
    "step_teardown:Teardown cluster"
)

# Combined steps for resume
ALL_STEPS=("${STEPS_PROJECT_ROOT[@]}" "${STEPS_WORKDIR[@]}")
PROJECT_ROOT_STEP_COUNT=${#STEPS_PROJECT_ROOT[@]}

list_steps() {
    echo "Available steps:"
    echo ""
    local total_steps=${#ALL_STEPS[@]}
    for ((i=0; i<total_steps; i++)); do
        local step_entry="${ALL_STEPS[$i]}"
        local step_name="${step_entry#*:}"
        printf "  %2d) %s\n" "$((i+1))" "$step_name"
    done
}

is_breakpoint() {
    local step_num=$1
    if [ -z "$BREAKPOINTS" ]; then
        return 1
    fi
    IFS=',' read -ra BP_ARRAY <<< "$BREAKPOINTS"
    for bp in "${BP_ARRAY[@]}"; do
        if [ "$bp" = "$step_num" ]; then
            return 0
        fi
    done
    return 1
}

run_from_step() {
    local start_step=${1:-0}
    local total_steps=${#ALL_STEPS[@]}
    local end_step=${2:-$total_steps}  # Optional end step (exclusive), defaults to all

    for ((i=start_step; i<end_step; i++)); do
        CURRENT_STEP=$i
        local step_entry="${ALL_STEPS[$i]}"
        local step_func="${step_entry%%:*}"
        local step_name="${step_entry#*:}"
        CURRENT_STEP_NAME="$step_name"

        echo ""
        echo "=========================================="
        echo "Step $((i+1))/$total_steps: $step_name"
        echo "=========================================="

        # Check for breakpoint
        if is_breakpoint "$((i+1))"; then
            echo ""
            echo ">>> BREAKPOINT at step $((i+1)): $step_name"
            read -p "Press Enter to continue..."
            echo ""
        fi

        # Determine which directory to run in and execute step
        # Disable errexit temporarily for controlled error handling
        local result
        local setup_kubectl_index=$((PROJECT_ROOT_STEP_COUNT + 2))  # step_setup_kubectl position
        set +e
        if [ $i -lt $PROJECT_ROOT_STEP_COUNT ]; then
            pushd "$PROJECT_ROOT" > /dev/null
            $step_func 2>&1 | tee "$STEP_LOG"
            result=${PIPESTATUS[0]}
            popd > /dev/null
        else
            pushd "$TEST_WORKDIR" > /dev/null
            # Only source env.sh for steps after step_setup_kubectl (index 5)
            # Earlier steps either don't need it or source it themselves
            if [ $i -gt $setup_kubectl_index ] && [ -f env.sh ]; then
                source env.sh
            fi
            $step_func 2>&1 | tee "$STEP_LOG"
            result=${PIPESTATUS[0]}
            popd > /dev/null
        fi
        set -e
        if [ $result -ne 0 ]; then
            FAILURE_COUNT=$((FAILURE_COUNT + 1))
            echo ""
            echo "=========================================="
            echo "FAILED: Step $((i+1)) - $step_name (exit code: $result)"
            echo "=========================================="
            {
                echo "============================================================"
                echo "FAILURE $FAILURE_COUNT: Step $((i+1)) - $step_name"
                echo "Exit code: $result"
                echo "Time: $(date)"
                echo "============================================================"
                echo ""
                echo "--- Step Output (last 100 lines) ---"
                tail -100 "$STEP_LOG"
                echo ""
                echo "--- Diagnostics ---"
                echo ">> Pods NOT Running/Completed:"
                kubectl get pods -A 2>/dev/null | awk 'NR==1 || ($4 != "Running" && $4 != "Completed")' || echo "(kubectl not available)"
                echo ""
                echo ">> CrashLoopBackOff pod logs (last 20 lines each):"
                local crash_pods
                crash_pods=$(kubectl get pods -A --no-headers 2>/dev/null | awk '$4 == "CrashLoopBackOff" {print $1 ":" $2}' || true)
                if [ -n "$crash_pods" ]; then
                    for pod_entry in $crash_pods; do
                        local ns="${pod_entry%%:*}"
                        local pod="${pod_entry#*:}"
                        echo ">>> $ns/$pod:"
                        kubectl logs -n "$ns" "$pod" --tail=20 2>/dev/null || echo "(logs unavailable)"
                        echo ""
                    done
                else
                    echo "(none)"
                fi
                echo ""
                echo ">> kubectl get events --sort-by=.lastTimestamp (last 20):"
                kubectl get events --sort-by=.lastTimestamp 2>/dev/null | tail -20 || echo "(kubectl not available)"
                echo ""
                echo ">> df -h:"
                df -h 2>/dev/null || true
                echo ""
            } >> "$FAILURE_LOG"
            echo ">>> Failure recorded — continuing to next step"
        fi
    done

    return 0
}

# Export variables for use in subshells (e.g., zsh session)
export PROJECT_ROOT TEST_WORKDIR

# ============================================================================
# Cleanup Function
# ============================================================================

cleanup() {
    local exit_code=$?

    # Success case - no failures and EXIT_OK
    if [ "$EXIT_OK" = true ] && [ "$FAILURE_COUNT" -eq 0 ]; then
        rm -f "$STEP_LOG" "$FAILURE_LOG"
        echo ""
        echo "=== All tests passed successfully ==="
        exit 0
    fi

    # Failure case - print failure log and prompt for teardown
    echo ""
    echo "=========================================="
    if [ "$FAILURE_COUNT" -gt 0 ]; then
        echo "=== $FAILURE_COUNT step(s) FAILED ==="
    else
        echo "=== Script exited unexpectedly ==="
    fi
    echo "=========================================="
    echo "=== Test work directory: $TEST_WORKDIR ==="

    if [ "$FAILURE_COUNT" -gt 0 ] && [ -f "$FAILURE_LOG" ]; then
        echo ""
        echo "=== Full Failure Log ==="
        cat "$FAILURE_LOG"
    fi

    echo ""
    echo "The cluster is still running. You can inspect it from another terminal."
    echo "Work directory: $TEST_WORKDIR"
    echo ""
    while true; do
        read -p "Type 'yes' to tear down the cluster, or Ctrl-C to keep it running: " confirm
        if [ "$confirm" = "yes" ]; then
            echo "=== Tearing down environment ==="
            (
                cd "$TEST_WORKDIR"
                if [ -f env.sh ]; then
                    source env.sh
                fi
                easy-db-lab down --yes
            )
            echo "=== Environment torn down ==="
            break
        else
            echo "Please type 'yes' to confirm teardown, or press Ctrl-C to exit and keep the cluster."
        fi
    done

    rm -f "$STEP_LOG" "$FAILURE_LOG"
    exit 1
}
trap cleanup EXIT

# ============================================================================
# Main Execution
# ============================================================================

# Handle --list-steps flag
if [ "$LIST_STEPS" = true ]; then
    list_steps
    EXIT_OK=true
    exit 0
fi

# Change to work directory
cd "$TEST_WORKDIR"
echo "=== Work directory: $(pwd) ==="

# Run all steps except teardown (last step).
# Teardown is handled by the cleanup trap — it prompts for confirmation
# when there are failures, or runs automatically on clean success.
TOTAL_STEPS=${#ALL_STEPS[@]}
LAST_STEP=$((TOTAL_STEPS - 1))  # Teardown is the last step
# --start-step is 1-based (matches --list-steps output), convert to 0-based index
FIRST_STEP=0
if [ -n "$START_STEP" ]; then
    FIRST_STEP=$((START_STEP - 1))
fi
run_from_step $FIRST_STEP $LAST_STEP

if [ "$FAILURE_COUNT" -eq 0 ]; then
    # All steps passed — run teardown directly
    step_teardown
    EXIT_OK=true
fi
# If FAILURE_COUNT > 0, cleanup trap will print failure log and prompt for teardown
