#!/bin/bash
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Disable AWS CLI pager to prevent hanging on output
export AWS_PAGER=""

CLUSTER_NAME="${1:-spark-bulk-test}"

echo "=== Building bulk-writer JAR ==="
(cd "$PROJECT_ROOT" && ./gradlew :bulk-writer:jar -q)
./gradlew shadowjar installdist

# Find the built JAR (version number is appended)
JAR_FILE=$(ls "$PROJECT_ROOT/bulk-writer/build/libs/bulk-writer-"*.jar 2>/dev/null | head -1)
if [ -z "$JAR_FILE" ]; then
    echo "Error: Could not find bulk-writer JAR"
    exit 1
fi
echo "Using JAR: $JAR_FILE"

echo "=== Initializing and provision cluster with Spark enabled ==="
easy-db-lab init "$CLUSTER_NAME" --spark.enable --clean --db 3 --app 0 --up

# Source env.sh for ssh wrapper (uses sshConfig)
if [ -f "$PROJECT_ROOT/env.sh" ]; then
    source "$PROJECT_ROOT/env.sh"
else
    echo "Error: env.sh not found after 'easy-db-lab up'"
    exit 1
fi

# Get datacenter from state.json
DC=$(jq -r '.initConfig.region' "$PROJECT_ROOT/state.json")
if [ -z "$DC" ] || [ "$DC" = "null" ]; then
    echo "Error: Could not read region from state.json"
    exit 1
fi
echo "Datacenter: $DC"

echo "=== Setting up Cassandra 5.0 ==="
easy-db-lab cassandra use 5.0

echo "=== Patching storage_compatibility_mode for Cassandra 5 bulk writer ==="
echo "storage_compatibility_mode: NONE" >> "$PROJECT_ROOT/cassandra.patch.yaml"
easy-db-lab cassandra update-config

echo "=== Starting Cassandra ==="
easy-db-lab cassandra start

echo "=== Getting Cassandra hosts ==="
# Get private IPs for all Cassandra nodes (hosts -c gives public IPs, we need private)
HOST_COUNT=$(easy-db-lab hosts -c | tr ',' '\n' | wc -l)
SIDECAR_HOSTS=""
for i in $(seq 0 $((HOST_COUNT - 1))); do
    IP=$(easy-db-lab ip db$i --private)
    if [ -n "$SIDECAR_HOSTS" ]; then
        SIDECAR_HOSTS="$SIDECAR_HOSTS,$IP"
    else
        SIDECAR_HOSTS="$IP"
    fi
done
echo "Hosts: $SIDECAR_HOSTS"

echo "=== Submitting bulk writer job ==="
# Configuration is passed via Spark properties (--conf)
# The bulk writer will automatically create the keyspace (NetworkTopologyStrategy) and table

# Enable command tracing for spark commands (useful for rerunning on failure)
set -x

# Capture spark submit result (don't exit on failure)
SPARK_EXIT_CODE=0
easy-db-lab spark submit \
  --jar "$JAR_FILE" \
  --main-class com.rustyrazorblade.easydblab.spark.DirectBulkWriter \
  --conf "spark.easydblab.sidecar.contactPoints=$SIDECAR_HOSTS" \
  --conf "spark.easydblab.keyspace=bulk_test" \
  --conf "spark.easydblab.table=data" \
  --conf "spark.easydblab.localDc=$DC" \
  --conf "spark.easydblab.rowCount=1000" \
  --conf "spark.easydblab.parallelism=4" \
  --conf "spark.easydblab.partitionCount=100" \
  --conf "spark.easydblab.replicationFactor=1" \
  --wait || SPARK_EXIT_CODE=$?

set +x

# Get cluster info for diagnostics
S3_BUCKET=$(jq -r '.s3Bucket' "$PROJECT_ROOT/state.json")
EMR_CLUSTER_ID=$(jq -r '.emrCluster.clusterId' "$PROJECT_ROOT/state.json")

if [ $SPARK_EXIT_CODE -ne 0 ]; then
    echo ""
    echo "=== SPARK JOB FAILED (exit code: $SPARK_EXIT_CODE) ==="
    echo ""

    # Get the most recent step ID from the EMR cluster
    echo "Fetching failed step details..."
    STEP_INFO=$(aws emr list-steps --cluster-id "$EMR_CLUSTER_ID" --step-states FAILED --max-items 1 2>/dev/null || echo "")

    if [ -n "$STEP_INFO" ]; then
        STEP_ID=$(echo "$STEP_INFO" | jq -r '.Steps[0].Id // empty')

        if [ -n "$STEP_ID" ]; then
            echo "Failed Step ID: $STEP_ID"
            echo ""

            # Download and display stderr.gz
            STDERR_S3_PATH="s3://$S3_BUCKET/spark/emr-logs/$EMR_CLUSTER_ID/steps/$STEP_ID/stderr.gz"
            echo "=== Waiting for EMR to upload logs to S3... ==="

            # Wait up to 60 seconds for logs to appear
            STDERR_TMP=$(mktemp)
            LOG_FOUND=false
            for i in {1..12}; do
                if aws s3 cp "$STDERR_S3_PATH" "$STDERR_TMP" 2>/dev/null; then
                    LOG_FOUND=true
                    break
                fi
                echo "  Waiting for logs... (attempt $i/12)"
                sleep 5
            done

            if [ "$LOG_FOUND" = true ]; then
                echo ""
                echo "=== STDERR CONTENTS (last 100 lines) ==="
                gunzip -c "$STDERR_TMP" | tail -100
                echo "=== END STDERR ==="
                rm -f "$STDERR_TMP"

                # Also show controller log for additional context
                CONTROLLER_S3_PATH="s3://$S3_BUCKET/spark/emr-logs/$EMR_CLUSTER_ID/steps/$STEP_ID/controller.gz"
                CONTROLLER_TMP=$(mktemp)
                if aws s3 cp "$CONTROLLER_S3_PATH" "$CONTROLLER_TMP" 2>/dev/null; then
                    echo ""
                    echo "=== CONTROLLER LOG ==="
                    gunzip -c "$CONTROLLER_TMP"
                    echo "=== END CONTROLLER ==="
                    rm -f "$CONTROLLER_TMP"
                fi
            else
                echo ""
                echo "Warning: Logs not available after 60 seconds"
                echo "Try manually: aws s3 cp $STDERR_S3_PATH - | gunzip"
            fi
        fi
    fi

    SPARK_FAILED=true
else
    echo "=== Verifying data ==="
    easy-db-lab cassandra cql "SELECT COUNT(*) FROM bulk_test.data;"
    echo ""
    echo "=== Sample data ==="
    easy-db-lab cassandra cql "SELECT * FROM bulk_test.data LIMIT 10;"
    SPARK_FAILED=false
fi

echo "=== Test complete ==="

