receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu:
      disk:
      load:
      filesystem:
      memory:
      network:
      processes:
  prometheus:
    config:
      scrape_configs:
        - job_name: 'clickhouse'
          scrape_interval: 5s
          static_configs:
            - targets: ['localhost:9363']
          relabel_configs:
            - target_label: instance
              replacement: '${env:HOSTNAME}:9363'
            - target_label: cluster
              replacement: '${env:CLUSTER_NAME}'
        - job_name: 'beyla'
          scrape_interval: 15s
          static_configs:
            - targets: ['localhost:9400']
          relabel_configs:
            - target_label: instance
              replacement: '${env:HOSTNAME}:9400'
            - target_label: cluster
              replacement: '${env:CLUSTER_NAME}'
        - job_name: 'ebpf-exporter'
          scrape_interval: 15s
          static_configs:
            - targets: ['localhost:9435']
          relabel_configs:
            - target_label: instance
              replacement: '${env:HOSTNAME}:9435'
            - target_label: cluster
              replacement: '${env:CLUSTER_NAME}'
        - job_name: 'cassandra-maac'
          scrape_interval: 15s
          static_configs:
            - targets: ['localhost:9000']
          relabel_configs:
            - target_label: instance
              replacement: '${env:HOSTNAME}:9000'
            - target_label: cluster
              replacement: '${env:CLUSTER_NAME}'
        - job_name: 'yace'
          scrape_interval: 5s
          scrape_timeout: 5s
          static_configs:
            - targets: ['localhost:5001']
          relabel_configs:
            - target_label: instance
              replacement: '${env:HOSTNAME}:5001'
            - target_label: cluster
              replacement: '${env:CLUSTER_NAME}'
  filelog/system:
    include:
      - /var/log/**/*.log
      - /var/log/messages
      - /var/log/syslog
    exclude:
      - /var/log/**/*.gz
      - /var/log/**/*.old
      - /var/log/easydblab/tools/*.log
    start_at: end
    include_file_path: true
    include_file_name: true
    attributes:
      source: system
  filelog/tools:
    include:
      - /var/log/easydblab/tools/*.log
    start_at: beginning
    include_file_path: true
    include_file_name: true
    attributes:
      source: tool-runner
  filelog/cassandra:
    include:
      - /mnt/db1/cassandra/logs/*.log
    exclude:
      - /mnt/db1/cassandra/logs/*.gz
    start_at: end
    include_file_path: true
    include_file_name: true
    attributes:
      source: cassandra
  # TODO: journald receiver removed — container lacks journalctl binary.
  # Re-add once we mount host journalctl or use a custom image.
  filelog/clickhouse-server:
    include:
      - /mnt/db1/clickhouse/logs/*.log
    start_at: end
    include_file_path: true
    include_file_name: true
    attributes:
      database: clickhouse
      component: server
    operators:
      - type: json_parser
        id: parse_json
        on_error: send
  filelog/clickhouse-keeper:
    include:
      - /mnt/db1/clickhouse/keeper/logs/*.log
    start_at: end
    include_file_path: true
    include_file_name: true
    attributes:
      database: clickhouse
      component: keeper
    operators:
      - type: json_parser
        id: parse_json
        on_error: send

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024
  memory_limiter:
    check_interval: 1s
    limit_mib: 256
    spike_limit_mib: 64
  resourcedetection:
    detectors: [system]
    system:
      hostname_sources: ["os"]
      resource_attributes:
        host.name:
          enabled: true
  # Extract node_role from K8s node label "type" (db, app) set during K3s agent join
  k8sattributes:
    passthrough: false
    extract:
      metadata:
        - k8s.node.name
      labels:
        - tag_name: node_role
          key: type
          from: node

exporters:
  prometheusremotewrite:
    endpoint: http://victoriametrics.default.svc.cluster.local:8428/api/v1/write
    tls:
      insecure: true
    resource_to_telemetry_conversion:
      enabled: true
  otlphttp/victorialogs:
    endpoint: http://victorialogs.default.svc.cluster.local:9428/insert/opentelemetry
    tls:
      insecure: true
  otlp/tempo:
    endpoint: tempo.default.svc.cluster.local:4320
    tls:
      insecure: true

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

service:
  extensions: [health_check]
  pipelines:
    # Local metrics — resourcedetection stamps host.name, k8sattributes adds node_role from node label
    metrics/local:
      receivers: [hostmetrics, prometheus]
      processors: [memory_limiter, resourcedetection, k8sattributes, batch]
      exporters: [prometheusremotewrite]
    # Remote metrics via OTLP (e.g., Spark nodes) — already have host.name and node_role
    metrics/otlp:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [prometheusremotewrite]
    logs/local:
      receivers: [filelog/system, filelog/tools, filelog/cassandra, filelog/clickhouse-server, filelog/clickhouse-keeper]
      processors: [memory_limiter, resourcedetection, k8sattributes, batch]
      exporters: [otlphttp/victorialogs]
    logs/otlp:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlphttp/victorialogs]
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp/tempo]
